{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM, DistilBertTokenizer, DistilBertModel,BertTokenizer, BertModel\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for computations\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Move model and tensors to GPU\n",
    "    print(\"Using GPU for computations\")\n",
    "else :\n",
    "    print(\"GPU unaviable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"medicalai/ClinicalBERT\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'DistilBertModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_length=100,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  # Explicit device assignment\n",
    "\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = \"i got stomach ache can you help me\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the output embeddings\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: [CLS] i got stomach ache can you help me [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Convert the last hidden states tensor to a numpy array for easier inspection\n",
    "last_hidden_states_np = last_hidden_states.cpu().numpy()\n",
    "\n",
    "# Convert the token IDs back to tokens using the tokenizer\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Get the indexes of the actual tokens (excluding special tokens like [CLS], [SEP], etc.)\n",
    "actual_token_indexes = [i for i, token in enumerate(tokens) if token != '[PAD]']\n",
    "\n",
    "# Extract the actual hidden states for actual tokens\n",
    "actual_hidden_states = last_hidden_states_np[0, actual_token_indexes]\n",
    "\n",
    "# Convert hidden states back to text\n",
    "decoded_text = tokenizer.decode(inputs['input_ids'][0][actual_token_indexes])\n",
    "\n",
    "# Print the decoded text\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Define your medical text\n",
    "text = \"i got stomach ache can you help me\"\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the text and convert it to tensors (for model input)\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get the sentence embedding\n",
    "with torch.no_grad():\n",
    "  # Pass the encoded input to the model\n",
    "  output = model(**encoded_input)\n",
    "\n",
    "# Access the pooled output (sentence embedding)\n",
    "sentence_embedding = output[0][:, 0, :]  # [CLS] token embedding for sentence representation\n",
    "\n",
    "# Print the sentence embedding size\n",
    "print(sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_chat(input_question):\n",
    "    # Tokenize input question\n",
    "    input_ids = tokenizer.encode(input_question, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    # Get answer span\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    # Find the answer span with the highest probability\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    # Decode answer tokens\n",
    "    answer_tokens = input_ids[0][answer_start:answer_end+1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "    \n",
    "    return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
