{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"jianghc/medical_chatbot\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(path)\n",
    "model = GPT2LMHeadModel.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = (\n",
    "    \"The conversation between human and AI assistant.\\n\"\n",
    "    \"[|Human|] {input}\\n\"\n",
    "    \"[|AI|]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = prompt_input.format_map({'input': \"i catch cold can you help me?\"})\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    beam_output = model.generate(**inputs,\n",
    "                                min_new_tokens=1, \n",
    "                                max_length=1024,\n",
    "                                num_beams=3,\n",
    "                                repetition_penalty=1.2,\n",
    "                                early_stopping=True,\n",
    "                                eos_token_id=198 \n",
    "                                )\n",
    "    print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
